#+TITLE: Language and Voice Laboratory Computing Resources

* Table of Contents                                               :TOC:QUOTE:
#+BEGIN_QUOTE
- [[#introduction][Introduction]]
- [[#scheduler][Scheduler]]
- [[#storage][Storage]]
- [[#containers][Containers]]
- [[#jupyter-notebooks-jupyterhub][Jupyter Notebooks (JupyterHub)]]
#+END_QUOTE

* Introduction
  The Language and Voice Laboratory (LVL) runs a tiny computing "cluster" called
  Terra.  This cluster consists of a few physical nodes, =terra=, =torpaq= and
  =gaia=.

  Access is granted by request by a sysadmin in the LVL.  Once you have a user
  account you can log into the main node:

  #+begin_src shell
  ssh user@terra.hir.is
  #+end_src

  Any questions additional questions can be asked on the =#terra= channel on
  [[https://romur.slack.com][Slack]].

* Scheduler
  The LVL cluster uses [[https://slurm.schedmd.org][Slurm]] to handle compute job scheduling and resource
  allocation.  All resource intensive tasks *must* use the scheduling system,
  and please refrain from requesting way more resources than is necessary.

  The command =sbatch= is used to submit batch jobs to the scheduler. This is
  the most common way to run tasks on the cluster. A batch job is described by a
  batch script and the command-line arguments to =sbatch=. 

  A batch script is a bash script with some special preprocessor directives, as
  seen in the example below.

  #+begin_src bash
  #!/bin/bash
  #SBATCH --gres=gpu:titanx:2
  #SBATCH --mem=12G
  #SBATCH --output=test-sbatch.log
  echo "I have these GPUs:" $CUDA_VISIBLE_DEVICES
  echo "On this machine" $(hostname)
  exit 0
  #+end_src

  We send this job to the scheduler with
  #+begin_src
  sbatch example-job.sbatch
  #+end_src

  This defines a job that will request two NVidia Titan X GPUs, 12 GB of memory
  and write stdout/stderr to the file =test-sbatch.log= in the current
  directory. Once the scheduler is able to allocate the necessary resources it
  will execute the job, writing the IDs of the allocated GPUs and the hostname
  of the allocated node to =test-sbatch.log=.

  We can use =sacct= to see the job history and =squeue= to see queued and
  running jobs.
    
   - [[./slurm-usage.org][Learn more]]
   - [[https://slurm.schedmd.com/pdfs/summary.pdf][Cheatsheet]]

* Storage
  There are a few file systems available on Terra. *None of these are backed
  up*. All, except =/scratch=, are raided for fault-tolerance.

  | Mount path   | Purpose                                                   | Size    | Speed                       | local node |
  |--------------+-----------------------------------------------------------+---------+-----------------------------+------------|
  | /data        | Shared datasets and archives. Read-only for users.        | 2.7 TiB | Fast reads & slow writes    | terra      |
  | /scratch     | "Unimportant" temporary files with many writes and reads. | 2 TiB   | Fastest                     | terra      |
  | /mnt/scratch | Links to /scratch for legacy reasons                      |         |                             |            |
  | /work        | More important temporary files                            | 3.4 TiB | Fastest reads & fast writes | torpaq     |
  | /home        | Code, configuration files, etc                            | 5.4T    | Slow                        | terra      |
  |--------------+-----------------------------------------------------------+---------+-----------------------------+------------|

* Containers
  [[https://sylabs.io/singularity/][Singularity]] ([[https://sylabs.io/singularity/faq/][FAQ]]) is a container solution for scientific computing that allows
  unprivileged use of containers. Singularity supports building its own images
  from scratch and ready-made Docker images.

  A user can build their own containerized application/project on there own
  machines which can be run on Terra in a Slurm batch job.

* Jupyter Notebooks (JupyterHub)
  Jupyter notebooks have become a popular way of doing scientific computing and
  interactive machine learning.

  LVL runs a JupyterHub accessible at https://terra.hir.is (RU intranet, you'll
  have to accept the self-signed cert) which allows users to spin up notebook
  servers through Slurm.

  The notebook server runs in a container using an image with a Python 3.7 Conda
  base environment. The /Conda/ tab allows you to create new environment, and
  new packages can be added to the envirment through the UI or in a notebook
  using the environment.

